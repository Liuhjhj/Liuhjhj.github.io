---
title: 微博爬虫的使用教程
date: 2021-03-28 16:26:08
summary: 本篇博客是针对https://github.com/dataabc/weiboSpider的新手使用教程
tags: 
	- 爬虫
	- 微博
---

# [微博爬虫](https://github.com/dataabc/weiboSpider)的使用教程

### 安装程序

本程序提供两种安装方式，一种是**源码安装**，另一种是**pip安装**，二者功能完全相同。如果你需要修改源码，建议使用第一种方式，否则选哪种安装方式都可以。

1. **源码安装**

   可以使用git clone或直接从网页下载zip并解压到本地

```sh
git clone https://github.com/dataabc/weiboSpider.git

# 如果你需要之后使用Pycharm打开这个软件的项目,可暂时先不输入这条命令
pip install -r requirements.txt
```

![点击红框下载](/blog_png/微博爬虫的使用/github下载.png)

2. **pip安装**

```shell
python3 -m pip install weibo-spider
```



### 打开项目

以**源码安装**的方式才能打开项目，使用Pycharm可以打开本项目

可以在Pycharm的终端内运行此命令

```sh
pip install -r requirements.txt
```



### 程序设置

**源码下载安装**的用户在weiboSpider目录下运行如下命令，**pip安装**的用户在任意有写权限的目录运行如下命令：

```shell
python3 -m weibo_spider
```

或在**Pycharm**中直接运行spider.py内的主函数

![主函数](/blog_png/微博爬虫的使用/点击绿色箭头运行.png)

第一次运行会生成config.json文件,并提示你需要配置它

```json
{
    "user_id_list": ["1669879400"],	//你想爬取的微博用户的ID
    "filter": 1,					//控制爬取范围
    "since_date": 10,				//设置爬取的微博的日期范围,这个为开始日期
    "end_date": "now",				//这个为结束日期
    "random_wait_pages": [1, 5],	//每爬取一定的页数暂停([1,5]表示每爬取1到5页暂停,页数是随机的)
    "random_wait_seconds": [6, 10],	//设置暂停的时间间隔([6,10]表示暂停时长在这个范围,时间是随机的)
    "global_wait": [[1000, 3600], [500, 2000]],
    //全局暂停时间([1000,3600]表示每爬取1000页后暂停3600秒),与上面的暂停机制共存
    "write_mode": ["mysql"],		//设置如何保存爬取到的数据
    "pic_download": 1,				//是否下载图片
    "video_download": 1,			//是否下载视频
	"file_download_timeout": [5, 5, 10],
	"result_dir_name": 0,			//设置保存结果的目录的名字(不包括路径)
    "cookie": "Cookie",				//设置Cookie
    "mysql_config": {				//配置mysql
        "host": "localhost",
        "port": 3306,
        "user": "root",
        "password": "258196",
        "charset": "utf8mb4"
    },
    "kafka_config": {
        "bootstrap-server": "127.0.0.1:9092",
        "weibo_topics": ["spider_weibo"],
        "user_topics": ["spider_weibo"]
    },
    "sqlite_config": "weibo.db"		//配置sqlite
}
```

下面解释每个参数的含义与用法



#### user_id_list

user_id_list是我们要爬取的微博的id，可以是一个，也可以是多个，例如：

```json
"user_id_list": ["1223178222", "1669879400", "1729370543"],
//上述代码代表我们要连续爬取user_id分别为“1223178222”、 “1669879400”、 “1729370543”的三个用户的微博
```

具体如何获取user_id：[点这里](https://github.com/dataabc/weiboSpider/blob/master/docs/userid.md)

user_id_list的值也可以是文件路径，我们可以把要爬的所有微博用户的user_id都写到txt文件里，然后把文件的位置路径赋值给user_id_list，**推荐这种方式**。
在txt文件中，每个user_id占一行，也可以在user_id后面加注释（可选），如用户昵称等信息，user_id和注释之间必需要有空格，文件名任意，类型为txt，位置位于本程序的同目录下，文件内容示例如下：

```txt
1223178222 胡歌
1669879400 迪丽热巴
1729370543 郭碧婷
```

假如文件叫user_id_list.txt，则user_id_list设置代码为：

```json
"user_id_list": "user_id_list.txt",
```



#### filter

filter控制爬取范围，值为1代表爬取全部原创微博，值为0代表爬取全部微博（原创+转发）。例如，如果要爬全部原创微博，请像下面代码一样设置：

```json
//爬全部原创微博
"filter": 1,
```



#### since_date

since_date值可以是日期，也可以是整数。如果是日期，代表爬取该日期之后的微博，格式应为“yyyy-mm-dd”，

如：

```json
"since_date": "2018-01-01",
```

代表爬取从2018年1月1日到现在的微博。
如果是整数，代表爬取最近n天的微博，如：

```json
//如果是日期应加上双引号,而数字没有双引号
"since_date": 10,
```

代表爬取最近10天的微博，这个说法不是特别准确，准确说是爬取发布时间从**10天前到本程序开始执行时**之间的微博。

**since_date是所有user的爬取起始时间，非常不灵活。如果你要爬多个用户，并且想单独为每个用户设置一个since_date，可以使用[定期自动爬取微博](https://github.com/dataabc/weiboSpider/blob/master/docs/automation.md)方法二中的方法，该方法可以为多个用户设置不同的since_date，非常灵活。**



#### end_date

end_date值可以是日期，也可以是"now"。如果是日期，代表爬取该日期之前的微博，格式应为“yyyy-mm-dd”；如果是"now"，代表爬取发布日期从since_date到现在的微博。since_date配合end_date，表示爬取发布日期在since_date和end_date之间的微博，包含边界。since_date是起始日期，end_date是结束日期，因此end_date时间应晚于since_date。注意，since_date即可以通过config.json文件的since_date参数设置，也可以通过user_id_list.txt设置；而end_date只能通过config.json文件的end_date参数设置，是全局变量，所有user_id都使用同一个end_date。
**推荐使用"now"作为end_date值**，当值为"now"时，获取结果是正确和稳定的；当end_date值不是"now"时，在爬微博数非常多的账号时，程序可能不稳定，得到很多空微博页，并且此时无法获取微博中的视频，如果想要获取视频，请为end_date赋值为"now"。



#### random_wait_pages

random_wait_pages值是一个长度为2的整数列表，代表每爬取x页微博暂停一次，x为整数，值在random_wait_pages列表两个整数之间随机获取。默认值为[1, 5]，代表每爬取1到5页暂停一次，如果程序被限制，可以加快暂停频率，即适当减小random_wait_pages内的值。



#### random_wait_seconds

random_wait_seconds值是一个长度为2的整数列表，代表每次暂停sleep x 秒，x为整数， 值在random_wait_seconds列表两个整数之间随机获取。默认值为[6, 10]，代表每次暂停sleep 6到10秒，如果程序被限制，可以增加等待时间，即适当增大random_wait_seconds内的值。



#### global_wait

global_wait控制全局等待时间，默认值为[[1000, 3600], [500, 2000]]，代表获取1000页微博，程序一次性暂停3600秒；之后获取500页微博，程序再一次性暂停2000秒；之后如果再获取1000页微博，程序一次性暂停3600秒，以此类推。默认的只有前面的两个全局等待时间（[1000, 3600]和[500, 2000]），可以设置多个，如值可以为[[1000, 3600], [500, 3000], [700, 3600]]，程序会根据配置依次等待对应时间，如果配置全部被使用，程序会从第一个配置开始，依次使用，循环往复。



#### write_mode

write_mode控制爬虫的结果如何处理，取值范围是csv、txt、json、mongo、mysql和sqlite，分别代表将结果文件写入csv、txt、json、MongoDB、MySQL和SQLite数据库。write_mode可以同时包含这些取值中的一个或几个，如：

```json
//表示将爬取的结果存入MySQL数据库的同时还写入本地的TXT文件
"write_mode": ["mysql", "txt"],
```



#### pic_download

pic_download控制是否下载微博中的图片，值为1代表下载，值为0代表不下载，如：

```json
//不下载爬取到的微博图片
"pic_download": 0,
```



#### video_download

video_download控制是否下载微博中的视频，值为1代表下载，值为0代表不下载，如：

```json
//下载爬取到的微博视频
"video_download": 1,
```



#### result_dir_name

result_dir_name控制保存结果目录的**名字**(不包括目录的路径)，可选值为0和1，默认值为0：

```json
"result_dir_name": 0,
```

值为0表示将结果文件保存在以用户昵称为名的文件夹里，这样结果更清晰；值为1表示将结果保存在以用户id为名的文件夹里，这样更能保证多次爬取的一致性，因为用户昵称可以改变，用户id是不变的。



#### cookie

下面介绍如何获取Cookie

1. 用Chrome打开[微博](https://passport.weibo.cn/signin/login)

   ![微博网页](https://camo.githubusercontent.com/50c7f33dbc7ebf32e1350e086906bca36ac0e9299688db73589465c87681a9d4/68747470733a2f2f706963747572652e636f676e697a652e6d652f636f676e697a652f6769746875622f776569626f7370696465722f636f6f6b6965312e706e67)

2. 输入用户名和密码,并完成相应的验证(验证码验证和短信验证)

3. 成功登录后会跳转到[m.weibo.cn](https://m.weibo.cn/)

4. 按F12键打开Chrome开发者工具，在地址栏输入并跳转到[weibo.cn](https://weibo.cn/)，跳转后会显示如下类似界面：

   ![Chrome](/blog_png/微博爬虫的使用/Chrome.png)

5. 按如图所示点击

   ![点击步骤](/blog_png/微博爬虫的使用/点击步骤.png)

6. 复制如上图所示的Cookie,并粘贴到**config.json**里面，如：

```json
"cookie": "Your Cookie",	
```



#### mysql_config

mysql_config控制mysql参数配置。如果你不需要将结果信息写入mysql，这个参数可以忽略，即删除或保留都无所谓；如果你需要写入mysql且config.json文件中mysql_config的配置与你的mysql配置不一样，请将该值改成你自己mysql中的参数配置。

需要安装pymysql包

```sh
pip install pymysql
```



#### sqlite_config

sqlite_config控制SQLite参数配置，代表SQLite数据库的保存路径，可根据自己需求修改。



### 运行程序

如果你已经按照上面的步骤配置好了，则可以直接再一次运行程序，就能正常爬取微博数据了

